# **LLM Exploration with Ollama and Streamlit**  

This is a web application built using **Ollama** and **Streamlit** to explore large language models (LLMs) like **deepseek-r1** and **llama3** locally.  

## **Usage**  

### **1. Clone the Repository**  
```sh
git clone https://github.com/ijeendu/run-llms-locally-using-ollama-streamlit.git
cd run-llms-locally-using-ollama-streamlit
```

### **2. Create a Virtual Environment**  
```sh
pyenv virtualenv 3.12 ollama-streamlit
pyenv activate ollama-streamlit
```

### **3. Install Dependencies**  
```sh
pip install -r requirements.txt
```

### **4. Ensure You Have Ollama Installed**  
- Download and install **Ollama** for your operating system from [ollama.com](https://ollama.com).  
- Make sure you have enough disk space to store the required models.  

### **5. Run the Application**  
```sh
streamlit run ollama-streamlit.py
```

Now, youâ€™re ready to explore LLMs locally! ðŸš€  

